---
layout: post
title: GVRNN
subtitle: Graph Variational Recurrent Neural Networks
---

이것은 2022 MLSA Workshop에 제출한 "Evaluation of creating scoring opportunities for teammates in soccer via trajectory prediction"논문을 바탕으로 제가 공부한 GVRNN에 대한 설명입니다.

- 위 논문은 GVRNN을 활용하여 선수들의 trajectory를 예측하는 문제를 다루고 있습니다. 논문에 대해서는 본 blog에서 설명하지 않고 GVRNN에 대해서만 설명합니다. 
- GVRNN을 설명하려면, 결국 AE(AutoEncoder), VAE(Variational AutoEncoder), VRNN(Variational Recurrent Neural Network), GNN(Graph Neural Network)를 모두 알아야한다. GVRNN은 VRNN과 GNN를 활용한 기법이므로 4가지 특징을 모두 안다면, GVRNN를 이해할 수 있을 것이다.
- 4가지 개념을 하나의 blog에 담으려면 요약된 정보만을 설명할 수 밖에 없다. 나중에 조금 더 자세히 쓸 계획이다.

### AE(AutoEncoder)
- 오토인코더(AE)는 입력 데이터를 압축한 후 복원하여 representation learning(데이터의 표현을 학습)하는 비지도 학습 알고리즘이다.
  
      1. Encoder : 입력 데이터를 내부 표현(잠재 공간)으로 변환 -> 추출된 특징을 Latent Vector라고 부름
  
      2. Decoder : Encoder를 거친 Latent Space를 받아 원본 데이터과 같은 형태로 재구성
  
- Model code github : [Model](https://github.com/dariocazzani/pytorch-AE/blob/master/models/AE.py)
  
![Model](https://blog.kakaocdn.net/dn/8JonH/btqFBec9cAF/mhxdDF930R0CrHs9NdUKv1/img.png)
  
### VAE(Variational AutoEncoder)
- 변이형 오토인코더(VAE)는 AE과 비슷한 구조를 가지지만, 확률 분포를 모델링한다는 점에서 차이가 있다

      1. Encoder : 입력 데이터를 내부 표현(잠재 공간)으로 변환 -> 확률 분포를 정의하는 평균과 표준편차 출력
  
      2. Latent Space : AE과 다르게 VAE에서는 Latent Space에서 noise를 추가함(동일한 데이터 생성 방지)
      * 정규분포로 부터 하나의 noise를 샘플링한 후 이를 바탕으로 Latent vector z를 얻는데, 이를 reparameterize이라 한다.
  
      3. Decoder : Latend Space를 거친 z를 받아 원본 데이터과 같은 형태로 재구성
  
- Model code github : [Model](https://github.com/dariocazzani/pytorch-AE/blob/master/models/VAE.py)
  
![Model](https://blog.kakaocdn.net/dn/b30Uzl/btrxY4wKngj/SucVwitDrRtQvi1xTHdrR0/img.png)

### VRNN
* RNN의 시간적 동적 특성과 VAE의 확률적 생성 모델링를 결합했다. 시간에 따라 변화하는 Trajectory를 효과적으로 학습하기 위해서 RNN도입

      1. Prior : 데이터를 접근하기 전 가지고 있는 사전 분포를 통해서 데이터를 추정함.
          1. Encoder가 입력데이터를 받아 Latent Space표현으로 변환하는 역할을 한다면, Prior은 Latent Space에 대한 전체적인 구조            와 분포를 정의함으로써 데이터를 생성할 때 일반화능력을 향상시킬 수 있다.
          2.수식은 t시점 이전(과거)의 정보만을 활용한 분포를 추정하는 식임을 확인할 수 있다.

$$\ \text{p}_{\theta}(z_t | x_{<t}, z_{<t}) = \ \phi_{\text{prior}}(h_{t-1})$$

      2. Latent Space : VAE과 같은 역할이다. 학습할 때는 Encoder의 확률 분포를 받고, 데이터 생성할 때는 Prior의 확률분포를 받는다.
  
      3. Encoder(Inference) : 입력 데이터를 내부 표현(잠재 공간)으로 변환 -> 학습할 때 사용

$$\ \text{q}_{\theta}(z_t | x_{\leq t}, z_{<t}) = \ \phi_{\text{enc}}(x_t,h_{t-1})$$

      4. Decoder(Generation) : Latend Space를 거친 z를 받아 원본 데이터과 같은 형태로 재구성

$$\ \text{p}_{\theta}(z_t | z_{\leq t}, x_{<t}) = \ \phi_{\text{dec}}(z_t,h_{t-1})$$

      5. Recurrence : 이전 시점의 hidden state과 입력데이터, Latent Vecor를 활용하여 현재 시점의 hidden state를 업데이터하는 과정이다
      
$$\ \text{h}_t = f(x_t, z_t, h_{t-1})$$

- Loss : VAE의 손실 함수는 주로 두 부분으로 구성됩니다: 재구성 손실(reconstruction loss)과 정규화 손실(Kullback-Leibler divergence). 이 두 요소를 합쳐 Evidence Lower Bound (ELBO)라고 하며, VAE의 목표는 ELBO를 최대화하는 것입니다.
  
      1. Reconstruction Loss
            - 입력 데이터와 출력 데이터 간의 차이를 줄입니다. 모델이 데이터를 얼마나 잘 재구성하는지 측정하는 지표
  
      2. Regularization Loss
            - 모델이 단순히 데이터를 재생산하는 것을 넘어서, 일반적인 데이터 패턴을 이해하고 새로운 데이터를 생성할 수 있게 하는 정규화 역할(입력 데이터의 복사본 생성 방지)
            - KL divergence는 Trajectory관점에서 다음 위치를 예측할 때, 이전 위치와 동일한 위치를 생성하지 않도록 하기 위한 것과 유사합니다. 즉, 모델이 데이터의 다양성을 유지하고 예측 가능한 패턴을 학습하도록 유도합니다.

- Model code link : [Model](https://github.com/emited/VariationalRecurrentNeuralNetwork/blob/master/model.py)
  
![image](https://github.com/GunHeeJoe/GunHeeJoe.github.io/assets/112679136/19c89399-9ba1-463e-8867-ea61078dec90)


  

### GVRNN
- GVRNN은 VRNN에 GNN기법을 추가한 것이다. 축구의 경우 각 선수들의 Trajectory는 다른 선수들의 영향을 받기 때문에 GNN기법도 추가한 것이다. -> 다중 에이전트의 상호작용을 학습
- Prior, Encoder(Inference), Decoder(Generation)를 통해 나온 확률분포에 GNN를 추가하므로써 선수들의 상호작용도 학습하는 구조
- Model code link : [Model](https://github.com/keisuke198619/C-OBSO/blob/main/vrnn/models/gvrnn.py)
  
![Model](https://github.com/GunHeeJoe/GunHeeJoe.github.io/assets/112679136/605202a0-3cf4-422e-87f5-fa1f8932cfcb)


