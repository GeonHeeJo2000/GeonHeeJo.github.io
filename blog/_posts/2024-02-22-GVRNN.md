---
layout: post
title: GVRNN
subtitle: Graph Variational Recurrent Neural Networks
---

이것은 2022 MLSA Workshop에 제출한 "Evaluation of creating scoring opportunities for teammates in soccer via trajectory prediction"논문을 바탕으로 제가 공부한 GVRNN에 대한 설명입니다.

- 위 논문은 GVRNN을 활용하여 선수들의 trajectory를 예측하는 문제를 다루고 있습니다. 논문에 대해서는 본 blog에서 설명하지 않고 GVRNN에 대해서만 설명합니다. 
- GVRNN을 설명하려면, 결국 AE(AutoEncoder), VAE(Variational AutoEncoder), VRNN(Variational Recurrent Neural Network), GNN(Graph Neural Network)를 모두 알아야한다. GVRNN은 VRNN과 GNN를 활용한 기법이므로 4가지 특징을 모두 안다면, GVRNN를 이해할 수 있을 것이다.
- 4가지 개념을 하나의 blog에 담으려면 요약된 정보만을 설명할 수 밖에 없다. 나중에 조금 더 자세히 쓸 계획이다.

### AE(AutoEncoder)
- 오토인코더(AE)는 입력 데이터를 압축한 후 복원하여 representation learning(데이터의 표현을 학습)하는 비지도 학습 알고리즘이다.
- Encoder : 입력 데이터를 내부 표현(잠재 공간)으로 변환 -> 추출된 특징을 Latent Vector라고 부름
- Decoder : Encoder를 거친 Latent Space를 받아 원본 데이터과 같은 형태로 재구성
- Model code github : [Model](https://github.com/dariocazzani/pytorch-AE/blob/master/models/AE.py)
  
![Model](https://blog.kakaocdn.net/dn/8JonH/btqFBec9cAF/mhxdDF930R0CrHs9NdUKv1/img.png)
  
### VAE(Variational AutoEncoder)
- 변이형 오토인코더(VAE)는 AE과 비슷한 구조를 가지지만, 확률 분포를 모델링한다는 점에서 차이가 있다
- Encoder : 입력 데이터를 내부 표현(잠재 공간)으로 변환 -> 확률 분포를 정의하는 평균과 표준편차 출력
- Latent Space : AE과 다르게 VAE에서는 Latent Space에서 noise를 추가함(동일한 데이터 생성 방지) -> 정규분포로 부터 하나의 noise를 샘플링한 후 이를 바탕으로 Latent vector z를 얻는데, 이를 reparameterize이라 한다.
- Decoder : Latend Space를 거친 z를 받아 원본 데이터과 같은 형태로 재구성
- Model code github : [Model](https://github.com/dariocazzani/pytorch-AE/blob/master/models/VAE.py)
  
![Model](https://blog.kakaocdn.net/dn/b30Uzl/btrxY4wKngj/SucVwitDrRtQvi1xTHdrR0/img.png)

### VRNN
- RNN의 시간적 동적 특성과 VAE의 확률적 생성 모델링를 결합했다. 시간에 따라 변화하는 Trajectory를 효과적으로 학습하기 위해서 RNN도입
- Prior : 데이터를 접근하기 전 가지고 있는 사전 분포를 통해서 데이터를 추정함. Encoder가 입력데이터를 받아 Latent Space표현으로 변환하는 역할을 한다면, Prior은 Latent Space에 대한 전체적인 구조와 분포를 정의함으로써 데이터를 생성할 때 일반화능력을 향상시킬 수 있다. 수식은 t시점 이전(과거)의 정보만을 활용한 분포를 추정하는 식임을 확인할 수 있다. $$𝑝_𝜃 (𝑧_𝑡│𝑥_{<𝑡}, 𝑧_{<𝑡} )= 𝜑_{𝑝𝑟𝑖𝑜𝑟} (ℎ_{𝑡−1})$$

- Latent Space : VAE과 같은 역할이다. 학습할 때는 Encoder의 확률 분포를 받고, 데이터 생성할 때는 Prior의 확률분포를 받는다.
- Encoder : 입력 데이터를 내부 표현(잠재 공간)으로 변환 -> 학습할 때 사용
- Decoder : Latend Space를 거친 z를 받아 원본 데이터과 같은 형태로 재구성
- Loss :
$$
- \mathbb{E}_{q_\phi(\mathbf{z}_{\leq T}|\mathbf{x}_{\leq T})} \left[ \sum_{t=1}^{T} \log p_\theta(\mathbf{x}_t \mathbf{z}_{\leqT}, \mathbf{x}_{<t}) \right] - \mathcal{D}_{KL} \left( q_\phi(\mathbf{z}_t | \mathbf{x}_{\leq T}, \mathbf{z}_{<t}) || p_\theta(\mathbf{z}_t | \mathbf{x}_{<t}, \mathbf{z}_{<t}) \right)
$$




- Model code link : [Model](https://github.com/emited/VariationalRecurrentNeuralNetwork/blob/master/model.py)
  
![Model](https://blog.kakaocdn.net/dn/8JonH/btqFBec9cAF/mhxdDF930R0CrHs9NdUKv1/img.png)

### GVRNN
- 오토인코더(AE)는 입력 데이터를 압축한 후 복원하여 representation learning(데이터의 표현을 학습)하는 비지도 학습 알고리즘이다.
- Encoder : 입력 데이터를 내부 표현(잠재 공간)으로 변환 -> 추출된 특징을 Latent Space라고 부름
- Decoder : Encoder를 거친 Latent Space를 받아 원본 데이터과 같은 형태로 재구성
- Model code link : ![Model](https://github.com/dariocazzani/pytorch-AE/blob/master/models/AE.py)
  
![Model](https://blog.kakaocdn.net/dn/8JonH/btqFBec9cAF/mhxdDF930R0CrHs9NdUKv1/img.png)

